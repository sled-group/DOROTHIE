<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomouse Driving Agents</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Anek+Devanagari&family=Nunito">
</head>
<body>
    <ul>
        <li>DOROTHIE</li>
    </ul>
    <div>
        <h1 class="lg-view">DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents</h1>
        <h1 class="sm-view">DOROTHIE</h1>
    </div>
    <div class="authors">
        <a href="" class="author">Ziqiao Ma,</a> <a href="" class="author">Ben VanDerPloeg,</a> <a href="" class="author">Cristian-Paul Bara,</a> 
        <a href="" class="author">Yidong Huang,</a> <a href="" class="author">Eui-In Kim,</a> <a href="" class="author">Felix Gervits,</a>
        <a href="" class="author">Matthew Marge,</a> <a href="" class="author">Joyce Chai</a>
    </div>
    <div class="conference">
        Conferece on Empirical Methods in Natural Language Processing (EMNLP 2022) 
        [<a href="https://arxiv.org/abs/2210.12511">pdf</a>][<a href="https://github.com/sled-group/DOROTHIE">code</a>]
    </div>
    <div class="section">
        <h2>Abstract</h2>
        <p>In the real world, autonomous driving agents navigate in highly dynamic environments full of unexpected situations 
            where pre-trained models are unreliable. In these situations, what is immediately available to vehicles is often 
            only human operators. Empowering autonomous driving agents with the ability to navigate in a continuous and dynamic 
            environment and to communicate with humans through sensorimotor-grounded dialogue becomes critical. To this end, we 
            introduce <b>Dialogue On the ROad To Handle Irregular Events (DOROTHIE)</b>, a novel interactive simulation platform that 
            enables the creation of unexpected situations on the fly to support empirical studies on situated communication with 
            autonomous driving agents. Based on this platform, we created the <b>Situated Dialogue Navigation (SDN)</b>, a navigation 
            benchmark of 183 trials with a total of 8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed 
            audio. SDN is developed to evaluate the agent's ability to predict dialogue moves from humans as well as generate its own 
            dialogue moves and physical navigation actions. We further developed a transformer-based baseline model for these 
            SDN tasks. Our empirical results indicate that language guided-navigation in a highly dynamic environment is an 
            extremely difficult task for end-to-end models. These results will provide insight towards future work on robust 
            autonomous driving agents.</p>
        <img src="images/DOROTHIE.jpg" alt="" id="DOROTHIE">
    </div>
    <div class="section">
        <h2>Dialogue On the ROad To Handle Irregular Events (DOROTHIE)</h2>
        <p><b>Dialogue On the ROad To Handle Irregular Events (DOROTHIE)</b> is a simulation framework developed upon CARLA 
            to study situated human-vehicle communication based on the <b>Wizard-of-Oz (WoZ)</b> setting. We extend the traditional 
            single-wizard to a duo-wizard setup approach by introducing a pair of Wizards. For detailed description of the simulation, 
            please refer to Appendex A in the paper.</p>
        <div class="section">
            <h3>Co-Wizard Interface</h3>
            <p>The <b>Cooperative-Wizard</b> controls the agent's behaviors and carries language communication with the human 
            participant to jointly achieve the goal. Down below is an example of Co-Wizard interface.</p>
            <img src="images/CO-WIZARD.jpg" alt="" id="CO-WIZARD">
        </div>
        
        <div class="section">
            <h3>Ad-Wizard Interface</h3>
            <p>The <b>Adversarial-Wizard</b> controls the environment and task interface and introduces unexpected situations 
            such as road blocks and goal manipulation on-the-fly.</p>
            <div class="AD-WIZARD-images">
                <img src="images/EXCEPTION_ENV.pdf" alt="" class = "AD-images">
                <img src="images/EXCEPTION-PLAN.pdf" alt="" class = "AD-images">
                <img src="images/EXCEPTION-GOAL.pdf" alt="" class = "AD-images">
            </div>
        </div>
    </div>
    <div class="section">
        <h2>Example</h3>
        <p>Down below is example dialogue with annotations. For details in annotation, please refer to Appendex B in the paper. 
        Each color bar represents a transaction unit and each box represents an exchange unit. The tasks challenge the agent to understand 
        the input dialogue move and imitate Co-Wizard's decision on the next navigation action and dialogue move to take.</p>
        <img src="images/EXAMPLE.pdf" alt="" id="EXAMPLE">
    </div>
    <div class="section">
        <h2>Temporally-Ordered Task-Oriented (TOTO) Transformer</h2>
        <p><b>Temporally-Ordered Task-Oriented Transformer (TOTO)</b> is temporally-ordered as it assigns sinusoidal temporal encodings 
        for input histroy instead of recurrent updates of hidden state, and is task-oriented as a unified architecture for 3 
        <b>Situated Dialogue Naviagation (SDN)</b> benchmark, Dialogue Understanding for Navigation (UfD), Dialogue Response for Navigation (RfN), 
        and Navigation from Dialogue (NfD). For detailed description of SDN task definition, please refer to Section 5 in the paper.</p>
        <img src="images/TOTO.jpg" alt="" id="TOTO">
    </div>
    <div class="section">
    <h2>Presentation Video</h2>
    </div>
    <div class="responsive-iframe">
        <iframe src="https://youtube.com/embed/2JJjYIWM26I" frameborder="0"></iframe>
    </div>
</body>
</html>